{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile imdb_spider_for_title.py\n",
    "import requests,re,os,sqlite3,sys,time,json\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imdb_config import *\n",
    "from urllib.parse import urljoin\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "\n",
    "class imdb_spider():\n",
    "    def __init__(self,dbpath=FILEPATH_DATABASE2, hd=None):\n",
    "        self.dbpath = dbpath\n",
    "        self.conn = sqlite3.connect(dbpath)\n",
    "        self.cur = self.conn.cursor()\n",
    "        if hd:\n",
    "            self.hd = hd\n",
    "        else:\n",
    "            self.hd = html_downloader(china=False)\n",
    "        self.filmlist_used_ttid()\n",
    "        self.used_url_li_tt = self.used_url_gen(FILEPATH_USEDURL_LI_TT)\n",
    "        self.used_url_title = self.used_url_gen(FILEPATH_USEDURL_TITLE)\n",
    "        self.error_url_title = self.used_url_gen(FILEPATH_ERROR_TITLE)\n",
    "        self.used_ttid_connection = self.used_url_gen(FILEPATH_USEDTTID_CONNECTION)\n",
    "        self.error_ttid_connection = self.used_url_gen(FILEPATH_ERRORTTID_CONNECTION)\n",
    "        \n",
    "    \n",
    "    def used_url_gen(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            tempstr = f.read()\n",
    "        return list(tempstr.split(','))\n",
    "    \n",
    "    def used_url_add(self, url, used_list, filename, success=True):\n",
    "        with open(filename, 'a+') as f:\n",
    "            f.write('{},'.format(url))\n",
    "        used_list.append(url)\n",
    "        if success:\n",
    "            print('SUCCESS: scrape {}'.format(url))\n",
    "        else:\n",
    "            print('ERROR: {}'.format(url))\n",
    "        \n",
    "    def filmlist_used_ttid(self):\n",
    "        try:\n",
    "            tempdf = pd.read_sql('select ttid from {}'.format(TABLENAME_FILMLIST), self.conn)\n",
    "            self.used_filmlist = set(tempdf['ttid'].values.tolist())\n",
    "        except:\n",
    "            self.used_filmlist = set()\n",
    "        \n",
    "    def parse_li_tt(self, res_content):\n",
    "        list_film_id = []\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        item_content = soup.find_all('div', {'class':'lister-item-content'})\n",
    "        for i in item_content:\n",
    "            item_header = i.find('h3', {'class':'lister-item-header'})\n",
    "            film_url = item_header.a['href']\n",
    "            film_name = item_header.a.get_text()\n",
    "            s_film_id = re.search(r'tt\\d+', film_url)\n",
    "            film_id = s_film_id.group()\n",
    "            try:\n",
    "                item_genre = i.find('span', {'class':'genre'})\n",
    "                film_genre = item_genre.get_text()\n",
    "                film_genre = re.sub(r'\\s','',film_genre)\n",
    "                film_year = i.find('span', {'class':'lister-item-year'}).get_text()\n",
    "                film_year = re.sub(r'\\D', '', film_year)\n",
    "            except:\n",
    "                print(\"ERROR: {}\".format(film_id))\n",
    "        # save\n",
    "            temp_dict = {'ttid':film_id, 'name':film_name, 'year':film_year, 'genre':film_genre, 'url':film_url}\n",
    "            list_film_id.append(temp_dict)\n",
    "        \n",
    "        # next page\n",
    "        item_next_page = soup.find('a', {'class':'lister-page-next'})\n",
    "        if item_next_page:\n",
    "            np_url = item_next_page['href']\n",
    "        else:\n",
    "            np_url = False\n",
    "        return list_film_id,np_url\n",
    "    def save_li_tt(self, list_film_id, to_db=True, table_name=TABLENAME_FILMLIST):\n",
    "        dict_for_pandas = {'ttid':[], 'name':[], 'year':[], 'genre':[], 'url':[]}\n",
    "        for i in list_film_id:\n",
    "            if i['ttid'] in self.used_filmlist:\n",
    "                continue\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(i.get(k))\n",
    "            self.used_filmlist.add(i['ttid'])\n",
    "        df_list_film = pd.DataFrame(dict_for_pandas)\n",
    "        \n",
    "        if to_db:\n",
    "            df_list_film.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_list_film\n",
    "    def scrapy_li_tt(self, genre_url,teststop=-1):\n",
    "        if teststop ==0:\n",
    "            print('test end')\n",
    "            return\n",
    "        if teststop > 0:\n",
    "            teststop = teststop-1\n",
    "        if genre_url in self.used_url_li_tt:\n",
    "            genre_url = self.used_url_li_tt[-2]\n",
    "            print('ATTENTION: start from {}'.format(genre_url))\n",
    "        response = self.hd.request_proxy(genre_url)\n",
    "        if response:\n",
    "            list_film_id,np_url = self.parse_li_tt(response.content)\n",
    "            df_list_film = self.save_li_tt(list_film_id)\n",
    "            df_list_film.to_csv(os.path.join(PATH_FILMLIST_TEMP,'{}.csv'.format(int(time.time()))), index=False)\n",
    "            self.used_url_add(genre_url,self.used_url_li_tt,FILEPATH_USEDURL_LI_TT)\n",
    "            if np_url:\n",
    "                np_url = urljoin(domain_url, np_url)\n",
    "                time.sleep(1)\n",
    "                self.scrapy_li_tt(np_url, teststop=teststop)\n",
    "        else:\n",
    "            print('ERROR: scrapy interrupt!!!')\n",
    "    \n",
    "    def scrapy_li_tt_all(self):\n",
    "        for i in genre_url_list:\n",
    "            self.scrapy_li_tt(i)\n",
    "    \n",
    "    def parse_title(self, res_content):\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        script_data = soup.find('script', {'type':'application/ld+json'})\n",
    "        if script_data:\n",
    "            dict_film_basic, df_film_crew = self.parse_title_json(script_data.get_text())\n",
    "        else:\n",
    "            dict_film_basic = {}\n",
    "            df_film_crew = pd.DataFrame()\n",
    "        plot_summary = soup.find('div',{'class':'summary_text'})\n",
    "        if plot_summary:\n",
    "            dict_film_basic['plot_summary'] = plot_summary.get_text()\n",
    "            dict_film_basic['plot_summary'] = re.sub('\\s\\s','',dict_film_basic['plot_summary'])\n",
    "        storyline = soup.find('div', {'class': 'inline canwrap'})\n",
    "        try:\n",
    "            dict_film_basic['storyline'] = storyline.p.span.get_text()\n",
    "            dict_film_basic['storyline'] = re.sub('\\s\\s','',dict_film_basic['storyline'])\n",
    "        except:\n",
    "            storyline = ''\n",
    "        \n",
    "        titleDetails = soup.find('div', {'class':'article', 'id':'titleDetails'})\n",
    "        s_budget = re.search(r'Budget:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_budget:\n",
    "            budget = s_budget.group()\n",
    "            s_budget = re.search(r'[\"$ ]+[\\d,]+',budget)\n",
    "            dict_film_basic['budget'] = s_budget.group()\n",
    "        \n",
    "        s_open_weekend_usa = re.search(r'Opening Weekend USA:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_open_weekend_usa:\n",
    "            open_weekend_usa = s_open_weekend_usa.group()\n",
    "            s_open_weekend_usa = re.search(r'[\"$ ]+[\\d,]+',open_weekend_usa)\n",
    "            dict_film_basic['open_weekend_usa'] = s_open_weekend_usa.group()\n",
    "        \n",
    "        s_gross_usa = re.search(r'Gross USA:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_gross_usa:\n",
    "            gross_usa = s_gross_usa.group()\n",
    "            s_gross_usa = re.search(r'[\"$ ]+[\\d,]+',gross_usa)\n",
    "            dict_film_basic['gross_usa'] = s_gross_usa.group()\n",
    "        \n",
    "        s_cumulative_worldwide_gross = re.search(r'Cumulative Worldwide Gross:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_cumulative_worldwide_gross:\n",
    "            cumulative_worldwide_gross = s_cumulative_worldwide_gross.group()\n",
    "            s_cumulative_worldwide_gross = re.search(r'[\"$ ]+[\\d,]+',cumulative_worldwide_gross)\n",
    "            dict_film_basic['cumulative_worldwide_gross'] = s_cumulative_worldwide_gross.group()\n",
    "        return dict_film_basic, df_film_crew\n",
    "    \n",
    "    def parse_title_json(self, json_str):\n",
    "        dict_film_basic = {}\n",
    "        tmp_dict = json.loads(json_str)\n",
    "        dict_film_basic['title_url'] = tmp_dict.get('url')\n",
    "        s_film_id = re.search(r'tt\\d+', tmp_dict.get('url'))\n",
    "        film_id = s_film_id.group()\n",
    "        dict_film_basic['ttid'] = film_id\n",
    "        dict_film_basic['name'] = tmp_dict.get('name')\n",
    "        dict_film_basic['genre'] = tmp_dict.get('genre')\n",
    "        if dict_film_basic['genre']:\n",
    "            dict_film_basic['genre'] = str(dict_film_basic['genre'])\n",
    "        dict_film_basic['contentRating'] = tmp_dict.get('contentRating')\n",
    "        dict_film_basic['description'] = tmp_dict.get('description')\n",
    "        dict_film_basic['datePublished'] = tmp_dict.get('datePublished')\n",
    "        dict_film_basic['keywords'] = tmp_dict.get('keywords')\n",
    "        agg_rating = tmp_dict.get('aggregateRating')\n",
    "        if agg_rating:\n",
    "            dict_film_basic['ratingCount'] = agg_rating.get('ratingCount')\n",
    "            dict_film_basic['bestRating'] = agg_rating.get('bestRating')\n",
    "            dict_film_basic['worstRating'] = agg_rating.get('worstRating')\n",
    "            dict_film_basic['ratingValue'] = agg_rating.get('ratingValue')\n",
    "        dict_for_pandas = {'ttid':[], 'film_name':[], 'type':[], 'person_name':[], 'person_url':[]}\n",
    "        list_film_multi = []\n",
    "        try:\n",
    "            actors = tmp_dict.get('actor')\n",
    "            if actors:\n",
    "                for i in actors:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'actor'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "            directors = tmp_dict.get('director')\n",
    "            if directors:\n",
    "                if type(directors) == dict:\n",
    "                    directors = [directors]\n",
    "                for i in directors:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'director'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "            creators = tmp_dict.get('creator')\n",
    "            if creators:\n",
    "                if type(creators) == dict:\n",
    "                    directors = [directors]\n",
    "                for i in creators:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'creator'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "\n",
    "            for i in list_film_multi:\n",
    "                for k in i.keys():\n",
    "                    dict_for_pandas[k].append(i.get(k))\n",
    "        except:\n",
    "            self.used_url_add(tmp_dict.get('url'), self.error_url_title, FILEPATH_ERROR_TITLE)\n",
    "            print(\"ERROR: {}\".format(tmp_dict.get('url')))\n",
    "        df_film_crew = pd.DataFrame(dict_for_pandas)\n",
    "        return dict_film_basic, df_film_crew\n",
    "    def scrape_title(self, title_url):\n",
    "        title_url = urljoin(domain_url, title_url)\n",
    "        if title_url in self.used_url_title:\n",
    "            return False\n",
    "        response = self.hd.request_proxy(title_url)\n",
    "        if response:\n",
    "            dict_film_basic, df_film_crew = self.parse_title(response.content)\n",
    "            df_film_basic = self.save_title_basic(dict_film_basic)\n",
    "            \n",
    "            df_film_crew.to_sql(name=TABLENAME_FILM_CREW,con=self.conn,if_exists='append',index=False)\n",
    "            self.used_url_add(title_url, self.used_url_title, FILEPATH_USEDURL_TITLE)\n",
    "            return df_film_basic, df_film_crew\n",
    "        else:\n",
    "            self.used_url_add(title_url, self.error_url_title, FILEPATH_ERROR_TITLE)\n",
    "            print('ERROR: {}'.format(title_url))\n",
    "            return False\n",
    "    def save_title_basic(self, dict_film_basic, to_db=True, table_name=TABLENAME_TITLE_BASIC):\n",
    "        dict_for_pandas = {'ttid':[],'name':[], 'title_url':[], 'genre':[],\n",
    "                          'contentRating':[],'datePublished':[],\n",
    "                          'ratingCount':[], 'bestRating':[], 'worstRating':[], 'ratingValue':[],\n",
    "                          'budget':[], 'open_weekend_usa':[], 'gross_usa':[], 'cumulative_worldwide_gross':[],\n",
    "                          'description':[], 'keywords':[], 'plot_summary':[], 'storyline':[]}\n",
    "        for k in dict_for_pandas.keys():\n",
    "            dict_for_pandas[k].append(dict_film_basic.get(k))\n",
    "        df_film_basic = pd.DataFrame(dict_for_pandas)\n",
    "        if to_db:\n",
    "            df_film_basic.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_film_basic\n",
    "    def scrape_title_list(self, urllist, teststop=-1):\n",
    "        for i in urllist:\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                return\n",
    "            if self.scrape_title(i):\n",
    "                if teststop>0:\n",
    "                    teststop = teststop-1\n",
    "                time.sleep(1)\n",
    "        print('mission complete')\n",
    "        \n",
    "    def parse_connection(self, res_content, film_ttid):\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        connection_content = soup.find('div', {'id':'connections_content'})\n",
    "        connection_list = connection_content.find('div', {'class':'list'})\n",
    "        connections = connection_list.contents\n",
    "        list_connection_data = []\n",
    "        ttype_list = ['TV Episode', 'Video', 'TV Movie', 'Short', 'TV Series']\n",
    "        for i in connections:\n",
    "            if not isinstance(i, NavigableString):\n",
    "                if i.name == 'a':\n",
    "                    tmp_type = i['id']\n",
    "                if i.name == 'div' and 'soda' in i['class']:\n",
    "                    tmp = i.contents\n",
    "                    tmp_dict = {'film_ttid':film_ttid, 'type':tmp_type}\n",
    "                    for j in tmp:\n",
    "                        if  j.name == 'a':\n",
    "                            tmp_name = j.get_text()\n",
    "                            tmp_name = re.sub(r'[\\s]',' ',tmp_name)\n",
    "                            if not tmp_dict.get('name'):\n",
    "                                tmp_dict['name'] = tmp_name\n",
    "                            tmp_url = j.get('href')\n",
    "                            if tmp_url:\n",
    "                                tmp_ttid = tmp_url.replace('/title/','')\n",
    "                                if not tmp_dict.get('url'):\n",
    "                                    tmp_dict['url'] = tmp_url\n",
    "                                    tmp_dict['ttid'] = tmp_ttid\n",
    "                            \n",
    "                        else:\n",
    "                            if re.search(r'\\d{4}', str(j)):\n",
    "                                s_tmp_date = re.search(r'\\d{4}', str(j))\n",
    "                                tmp_date = s_tmp_date.group()\n",
    "                                if not tmp_dict.get('year'):\n",
    "                                    tmp_dict['year'] = tmp_date\n",
    "                            for h in ttype_list:\n",
    "                                if h in str(j):\n",
    "                                    if not tmp_dict.get('ttype'):\n",
    "                                        tmp_dict['ttype'] = h\n",
    "                                \n",
    "                    list_connection_data.append(tmp_dict)\n",
    "        dict_for_pandas = {'film_ttid':[], 'type':[], 'name':[], 'ttid':[], 'url':[], 'year':[], 'ttype':[]}\n",
    "        for h in list_connection_data:\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(h.get(k))\n",
    "        df_connections = pd.DataFrame(dict_for_pandas)\n",
    "                \n",
    "        return df_connections\n",
    "    def scrapy_connections(self, ttid):\n",
    "        url = 'https://www.imdb.com/title/{}/movieconnections?ref_=tt_ql_trv_6'.format(ttid)\n",
    "        if ttid in self.used_ttid_connection:\n",
    "            return False\n",
    "        response = self.hd.request_proxy(url)\n",
    "        if response:\n",
    "            try:\n",
    "                df_connections = self.parse_connection(response.content, ttid)\n",
    "                df_connections.to_sql(name=TABLENAME_FILE_CONNECTIONS,con=self.conn,if_exists='append',index=False)\n",
    "                self.used_url_add(ttid, self.used_ttid_connection, FILEPATH_USEDTTID_CONNECTION)\n",
    "                return True,df_connections\n",
    "            except:\n",
    "                self.used_url_add(ttid, self.error_ttid_connection, FILEPATH_ERRORTTID_CONNECTION, success=False)\n",
    "                return False\n",
    "        else:\n",
    "            print('ERROR: {}'.format(ttid))\n",
    "            return False\n",
    "    def scrapy_connections_list(self, ttid_list, teststop=-1):\n",
    "        for i in ttid_list:\n",
    "            if teststop == 0:\n",
    "                print('test end')\n",
    "                break\n",
    "            if self.scrapy_connections(i):\n",
    "                if teststop>0:\n",
    "                    teststop = teststop -1\n",
    "        print('mission complete')\n",
    "    \n",
    "\n",
    "                        \n",
    "                            \n",
    "\n",
    "            \n",
    "# # scrape list          \n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     sp.scrapy_li_tt_all()\n",
    "\n",
    "# scrape title\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     title_urls = df1['url'].values.tolist()\n",
    "#     sp.scrape_title_list(title_urls)\n",
    "\n",
    "# scrape connections\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     ttids = df1['ttid'].values.tolist()\n",
    "#     sp.scrapy_connections_list(ttids)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perpare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = html_downloader(china=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = genre_url_list[0]\n",
    "t1 = imdb_spider(hd=hd)\n",
    "t1.scrapy_li_tt(url,teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t1.hd.ip_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_url = 'https://www.imdb.com/title/tt0120737/?ref_=adv_li_tt'\n",
    "r2 = hd.request_proxy(title_url)\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "d2 = t2.parse_title(r2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367396, 5)\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "cur = conn.cursor()\n",
    "df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_to_int(value):\n",
    "    s_tmp = re.search(r'\\d+',value)\n",
    "    if s_tmp:\n",
    "        tmp = s_tmp.group()\n",
    "        tmp = np.int(tmp)\n",
    "    else:\n",
    "        tmp = np.nan\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208134, 5)\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.copy()\n",
    "df2['year'] = df2['year'].apply(year_to_int)\n",
    "df2 = df2[(df2['year']>2000) & (df2['year']<2019)]\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "title_urls = df2['url'].values.tolist()\n",
    "t3.scrape_title_list(title_urls, teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2 = sqlite3.connect(FILEPATH_DATABASE2)\n",
    "dftitle_basic = pd.read_sql('select * from {}'.format(TABLENAME_TITLE_BASIC), conn2)\n",
    "dftitle_crew = pd.read_sql('select * from {}'.format(TABLENAME_FILM_CREW), conn2)\n",
    "print(dftitle_basic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftitle_basic.to_csv('/home/guijideanhao/buffer/data_preview_film_basic.csv', index=False)\n",
    "dftitle_crew.to_csv('/home/guijideanhao/buffer/data_preview_film_crew.csv', index=False)\n",
    "df2.to_csv('/home/guijideanhao/buffer/data_preview_film_list2000-2019part.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connection page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_url = 'https://www.imdb.com/title/tt1320253/movieconnections?ref_=tt_ql_trv_6'\n",
    "r4 = requests.get(connect_url)\n",
    "r4.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = imdb_spider(dbpath='test.db',hd=hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = t4.parse_connection(r4.content, 'tt1320253')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: scrape tt1706620\n",
      "SUCCESS: scrape tt0120737\n",
      "test end\n",
      "mission complete\n"
     ]
    }
   ],
   "source": [
    "ttids = df2['ttid'].values.tolist()\n",
    "t4.scrapy_connections_list(ttids, teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1661, 7)\n"
     ]
    }
   ],
   "source": [
    "df_connections = pd.read_sql('select * from {}'.format(TABLENAME_FILE_CONNECTIONS), t4.conn)\n",
    "print(df_connections.shape)\n",
    "df_connections.to_csv('/home/guijideanhao/buffer/data_preview_film_connections.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
