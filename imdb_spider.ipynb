{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting imdb_spider.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile imdb_spider.py\n",
    "import requests,re,os,sqlite3,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from imdb_config import *\n",
    "from urllib.parse import urljoin\n",
    "sys.path.append('/home/ubuntu/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "\n",
    "class imdb_spider():\n",
    "    def __init__(self,dbpath=FILEPATH_DATABASE, hd=None):\n",
    "        self.conn = sqlite3.connect(dbpath)\n",
    "        self.cur = self.conn.cursor()\n",
    "        if hd:\n",
    "            self.hd = hd\n",
    "        else:\n",
    "            self.hd = html_downloader(world=True)\n",
    "        self.filmlist_used_ttid()\n",
    "        self.used_url_li_tt = self.used_url_gen(FILEPATH_USEDURL_LI_TT)\n",
    "    \n",
    "    def used_url_gen(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            tempstr = f.read()\n",
    "        return list(tempstr.split(','))\n",
    "    \n",
    "    def used_url_add(self, url, used_list, filename):\n",
    "        with open(filename, 'a+') as f:\n",
    "            f.write('{},'.format(url))\n",
    "        used_list.append(url)\n",
    "        print('SUCCESS: scrape {}'.format(url))\n",
    "        \n",
    "    def filmlist_used_ttid(self):\n",
    "        try:\n",
    "            tempdf = pd.read_sql('select ttid from {}'.format(TABLENAME_FILMLIST), self.conn)\n",
    "            self.used_filmlist = set(tempdf['ttid'].values.tolist())\n",
    "        except:\n",
    "            self.used_filmlist = set()\n",
    "        \n",
    "    def parse_li_tt(self, res_content):\n",
    "        list_film_id = []\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        item_content = soup.find_all('div', {'class':'lister-item-content'})\n",
    "        for i in item_content:\n",
    "            item_header = i.find('h3', {'class':'lister-item-header'})\n",
    "            film_url = item_header.a['href']\n",
    "            film_name = item_header.a.get_text()\n",
    "            item_genre = i.find('span', {'class':'genre'})\n",
    "            film_genre = item_genre.get_text()\n",
    "            film_year = i.find('span', {'class':'lister-item-year'}).get_text()\n",
    "        # modify\n",
    "            s_film_id = re.search(r'tt\\d+', film_url)\n",
    "            film_id = s_film_id.group()\n",
    "            film_genre = re.sub(r'\\s','',film_genre)\n",
    "            film_year = re.sub(r'\\D', '', film_year)\n",
    "        # save\n",
    "            temp_dict = {'ttid':film_id, 'name':film_name, 'year':film_year, 'genre':film_genre, 'url':film_url}\n",
    "            list_film_id.append(temp_dict)\n",
    "        \n",
    "        # next page\n",
    "        item_next_page = soup.find('a', {'class':'lister-page-next'})\n",
    "        if item_next_page:\n",
    "            np_url = item_next_page['href']\n",
    "        else:\n",
    "            np_url = False\n",
    "        return list_film_id,np_url\n",
    "    def save_li_tt(self, list_film_id, to_db=True, table_name=TABLENAME_FILMLIST):\n",
    "        dict_for_pandas = {'ttid':[], 'name':[], 'year':[], 'genre':[], 'url':[]}\n",
    "        for i in list_film_id:\n",
    "            if i['ttid'] in self.used_filmlist:\n",
    "                continue\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(i.get(k))\n",
    "            self.used_filmlist.add(i['ttid'])\n",
    "        df_list_film = pd.DataFrame(dict_for_pandas)\n",
    "        \n",
    "        if to_db:\n",
    "            df_list_film.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_list_film\n",
    "    def scrapy_li_tt(self, genre_url,teststop=-1):\n",
    "        if teststop ==0:\n",
    "            print('test end')\n",
    "            return\n",
    "        if teststop > 0:\n",
    "            teststop = teststop-1\n",
    "        if genre_url in self.used_url_li_tt:\n",
    "            genre_url = self.used_url_li_tt[-2]\n",
    "            print('ATTENTION: start from {}'.format(genre_url))\n",
    "        response = self.hd.request_proxy(genre_url)\n",
    "        if response:\n",
    "            list_film_id,np_url = self.parse_li_tt(response.content)\n",
    "            df_list_film = self.save_li_tt(list_film_id)\n",
    "            df_list_film.to_csv(os.path.join(PATH_FILMLIST_TEMP,'{}.csv'.format(int(time.time()))), index=False)\n",
    "            self.used_url_add(genre_url,self.used_url_li_tt,FILEPATH_USEDURL_LI_TT)\n",
    "            if np_url:\n",
    "                np_url = urljoin(domain_url, np_url)\n",
    "                time.sleep(1)\n",
    "                self.scrapy_li_tt(np_url, teststop=teststop)\n",
    "        else:\n",
    "            print('ERROR: scrapy interrupt!!!')\n",
    "    \n",
    "    def scrapy_li_tt_all(self):\n",
    "        for i in genre_url_list:\n",
    "            self.scrapy_li_tt(i)\n",
    "\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     sp.scrapy_li_tt_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = html_downloader(world=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION: start from https://www.imdb.com/search/title/?title_type=feature&genres=action&start=451&explore=genres&ref_=adv_nxt\n",
      "SUCCESS: scrape https://www.imdb.com/search/title/?title_type=feature&genres=action&start=451&explore=genres&ref_=adv_nxt\n",
      "SUCCESS: scrape https://www.imdb.com/search/title/?title_type=feature&genres=action&start=501&explore=genres&ref_=adv_nxt\n",
      "test end\n"
     ]
    }
   ],
   "source": [
    "url = genre_url_list[0]\n",
    "t1 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "t1.scrapy_li_tt(url,teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t1.hd.ip_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), con = t1.conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
