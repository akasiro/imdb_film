{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile imdb_spider_for_title.py\n",
    "import requests,re,os,sqlite3,sys,time,json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imdb_config import *\n",
    "from urllib.parse import urljoin\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "\n",
    "class imdb_spider():\n",
    "    def __init__(self,dbpath=FILEPATH_DATABASE2, hd=None):\n",
    "        self.dbpath = dbpath\n",
    "        self.conn = sqlite3.connect(dbpath)\n",
    "        self.cur = self.conn.cursor()\n",
    "        if hd:\n",
    "            self.hd = hd\n",
    "        else:\n",
    "            self.hd = html_downloader(china=False)\n",
    "        self.filmlist_used_ttid()\n",
    "#         self.used_url_li_tt = self.used_url_gen(FILEPATH_USEDURL_LI_TT)\n",
    "        self.used_url_title = self.used_url_gen(FILEPATH_USEDURL_TITLE)\n",
    "        self.error_url_title = self.used_url_gen(FILEPATH_ERROR_TITLE)\n",
    "    \n",
    "    def used_url_gen(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            tempstr = f.read()\n",
    "        return list(tempstr.split(','))\n",
    "    \n",
    "    def used_url_add(self, url, used_list, filename):\n",
    "        with open(filename, 'a+') as f:\n",
    "            f.write('{},'.format(url))\n",
    "        used_list.append(url)\n",
    "        print('SUCCESS: scrape {}'.format(url))\n",
    "        \n",
    "    def filmlist_used_ttid(self):\n",
    "        try:\n",
    "            tempdf = pd.read_sql('select ttid from {}'.format(TABLENAME_FILMLIST), self.conn)\n",
    "            self.used_filmlist = set(tempdf['ttid'].values.tolist())\n",
    "        except:\n",
    "            self.used_filmlist = set()\n",
    "        \n",
    "    def parse_li_tt(self, res_content):\n",
    "        list_film_id = []\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        item_content = soup.find_all('div', {'class':'lister-item-content'})\n",
    "        for i in item_content:\n",
    "            item_header = i.find('h3', {'class':'lister-item-header'})\n",
    "            film_url = item_header.a['href']\n",
    "            film_name = item_header.a.get_text()\n",
    "            s_film_id = re.search(r'tt\\d+', film_url)\n",
    "            film_id = s_film_id.group()\n",
    "            try:\n",
    "                item_genre = i.find('span', {'class':'genre'})\n",
    "                film_genre = item_genre.get_text()\n",
    "                film_genre = re.sub(r'\\s','',film_genre)\n",
    "                film_year = i.find('span', {'class':'lister-item-year'}).get_text()\n",
    "                film_year = re.sub(r'\\D', '', film_year)\n",
    "            except:\n",
    "                print(\"ERROR: {}\".format(film_id))\n",
    "        # save\n",
    "            temp_dict = {'ttid':film_id, 'name':film_name, 'year':film_year, 'genre':film_genre, 'url':film_url}\n",
    "            list_film_id.append(temp_dict)\n",
    "        \n",
    "        # next page\n",
    "        item_next_page = soup.find('a', {'class':'lister-page-next'})\n",
    "        if item_next_page:\n",
    "            np_url = item_next_page['href']\n",
    "        else:\n",
    "            np_url = False\n",
    "        return list_film_id,np_url\n",
    "    def save_li_tt(self, list_film_id, to_db=True, table_name=TABLENAME_FILMLIST):\n",
    "        dict_for_pandas = {'ttid':[], 'name':[], 'year':[], 'genre':[], 'url':[]}\n",
    "        for i in list_film_id:\n",
    "            if i['ttid'] in self.used_filmlist:\n",
    "                continue\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(i.get(k))\n",
    "            self.used_filmlist.add(i['ttid'])\n",
    "        df_list_film = pd.DataFrame(dict_for_pandas)\n",
    "        \n",
    "        if to_db:\n",
    "            df_list_film.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_list_film\n",
    "    def scrapy_li_tt(self, genre_url,teststop=-1):\n",
    "        if teststop ==0:\n",
    "            print('test end')\n",
    "            return\n",
    "        if teststop > 0:\n",
    "            teststop = teststop-1\n",
    "        if genre_url in self.used_url_li_tt:\n",
    "            genre_url = self.used_url_li_tt[-2]\n",
    "            print('ATTENTION: start from {}'.format(genre_url))\n",
    "        response = self.hd.request_proxy(genre_url)\n",
    "        if response:\n",
    "            list_film_id,np_url = self.parse_li_tt(response.content)\n",
    "            df_list_film = self.save_li_tt(list_film_id)\n",
    "            df_list_film.to_csv(os.path.join(PATH_FILMLIST_TEMP,'{}.csv'.format(int(time.time()))), index=False)\n",
    "            self.used_url_add(genre_url,self.used_url_li_tt,FILEPATH_USEDURL_LI_TT)\n",
    "            if np_url:\n",
    "                np_url = urljoin(domain_url, np_url)\n",
    "                time.sleep(1)\n",
    "                self.scrapy_li_tt(np_url, teststop=teststop)\n",
    "        else:\n",
    "            print('ERROR: scrapy interrupt!!!')\n",
    "    \n",
    "    def scrapy_li_tt_all(self):\n",
    "        for i in genre_url_list:\n",
    "            self.scrapy_li_tt(i)\n",
    "    \n",
    "    def parse_title(self, res_content):\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        script_data = soup.find('script', {'type':'application/ld+json'})\n",
    "        if script_data:\n",
    "            dict_film_basic, df_film_crew = self.parse_title_json(script_data.get_text())\n",
    "        else:\n",
    "            dict_film_basic = {}\n",
    "            df_film_crew = pd.DataFrame()\n",
    "        plot_summary = soup.find('div',{'class':'summary_text'})\n",
    "        if plot_summary:\n",
    "            dict_film_basic['plot_summary'] = plot_summary.get_text()\n",
    "            dict_film_basic['plot_summary'] = re.sub('\\s\\s','',dict_film_basic['plot_summary'])\n",
    "        storyline = soup.find('div', {'class': 'inline canwrap'})\n",
    "        try:\n",
    "            dict_film_basic['storyline'] = storyline.p.span.get_text()\n",
    "            dict_film_basic['storyline'] = re.sub('\\s\\s','',dict_film_basic['storyline'])\n",
    "        except:\n",
    "            storyline = ''\n",
    "        \n",
    "        titleDetails = soup.find('div', {'class':'article', 'id':'titleDetails'})\n",
    "        s_budget = re.search(r'Budget:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_budget:\n",
    "            budget = s_budget.group()\n",
    "            s_budget = re.search(r'[\"$ ]+[\\d,]+',budget)\n",
    "            dict_film_basic['budget'] = s_budget.group()\n",
    "        \n",
    "        s_open_weekend_usa = re.search(r'Opening Weekend USA:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_open_weekend_usa:\n",
    "            open_weekend_usa = s_open_weekend_usa.group()\n",
    "            s_open_weekend_usa = re.search(r'[\"$ ]+[\\d,]+',open_weekend_usa)\n",
    "            dict_film_basic['open_weekend_usa'] = s_open_weekend_usa.group()\n",
    "        \n",
    "        s_gross_usa = re.search(r'Gross USA:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_gross_usa:\n",
    "            gross_usa = s_gross_usa.group()\n",
    "            s_gross_usa = re.search(r'[\"$ ]+[\\d,]+',gross_usa)\n",
    "            dict_film_basic['gross_usa'] = s_gross_usa.group()\n",
    "        \n",
    "        s_cumulative_worldwide_gross = re.search(r'Cumulative Worldwide Gross:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_cumulative_worldwide_gross:\n",
    "            cumulative_worldwide_gross = s_cumulative_worldwide_gross.group()\n",
    "            s_cumulative_worldwide_gross = re.search(r'[\"$ ]+[\\d,]+',cumulative_worldwide_gross)\n",
    "            dict_film_basic['cumulative_worldwide_gross'] = s_cumulative_worldwide_gross.group()\n",
    "        return dict_film_basic, df_film_crew\n",
    "    \n",
    "    def parse_title_json(self, json_str):\n",
    "        dict_film_basic = {}\n",
    "        tmp_dict = json.loads(json_str)\n",
    "        dict_film_basic['title_url'] = tmp_dict.get('url')\n",
    "        s_film_id = re.search(r'tt\\d+', tmp_dict.get('url'))\n",
    "        film_id = s_film_id.group()\n",
    "        dict_film_basic['ttid'] = film_id\n",
    "        dict_film_basic['name'] = tmp_dict.get('name')\n",
    "        dict_film_basic['genre'] = tmp_dict.get('genre')\n",
    "        if dict_film_basic['genre']:\n",
    "            dict_film_basic['genre'] = str(dict_film_basic['genre'])\n",
    "        dict_film_basic['contentRating'] = tmp_dict.get('contentRating')\n",
    "        dict_film_basic['description'] = tmp_dict.get('description')\n",
    "        dict_film_basic['datePublished'] = tmp_dict.get('datePublished')\n",
    "        dict_film_basic['keywords'] = tmp_dict.get('keywords')\n",
    "        agg_rating = tmp_dict.get('aggregateRating')\n",
    "        if agg_rating:\n",
    "            dict_film_basic['ratingCount'] = agg_rating.get('ratingCount')\n",
    "            dict_film_basic['bestRating'] = agg_rating.get('bestRating')\n",
    "            dict_film_basic['worstRating'] = agg_rating.get('worstRating')\n",
    "            dict_film_basic['ratingValue'] = agg_rating.get('ratingValue')\n",
    "        list_film_multi = []\n",
    "        actors = tmp_dict.get('actor')\n",
    "        if actors:\n",
    "            for i in actors:\n",
    "                tmp = {}\n",
    "                tmp['ttid'] = film_id\n",
    "                tmp['film_name'] = tmp_dict.get('name')\n",
    "                tmp['type'] = 'actor'\n",
    "                tmp['person_name'] = i.get('name')\n",
    "                tmp['person_url'] = i.get('url')\n",
    "                list_film_multi.append(tmp)\n",
    "        directors = tmp_dict.get('director')\n",
    "        if directors:\n",
    "            if type(directors) == dict:\n",
    "                directors = [directors]\n",
    "            for i in directors:\n",
    "                tmp = {}\n",
    "                tmp['ttid'] = film_id\n",
    "                tmp['film_name'] = tmp_dict.get('name')\n",
    "                tmp['type'] = 'director'\n",
    "                tmp['person_name'] = i.get('name')\n",
    "                tmp['person_url'] = i.get('url')\n",
    "                list_film_multi.append(tmp)\n",
    "        creators = tmp_dict.get('creator')\n",
    "        if creators:\n",
    "            if type(creators) == dict:\n",
    "                directors = [directors]\n",
    "            for i in creators:\n",
    "                tmp = {}\n",
    "                tmp['ttid'] = film_id\n",
    "                tmp['film_name'] = tmp_dict.get('name')\n",
    "                tmp['type'] = 'creator'\n",
    "                tmp['person_name'] = i.get('name')\n",
    "                tmp['person_url'] = i.get('url')\n",
    "                list_film_multi.append(tmp)\n",
    "        dict_for_pandas = {'ttid':[], 'film_name':[], 'type':[], 'person_name':[], 'person_url':[]}\n",
    "        for i in list_film_multi:\n",
    "            for k in i.keys():\n",
    "                dict_for_pandas[k].append(i.get(k))\n",
    "        df_film_crew = pd.DataFrame(dict_for_pandas)\n",
    "        return dict_film_basic, df_film_crew\n",
    "    def scrape_title(self, title_url):\n",
    "        title_url = urljoin(domain_url, title_url)\n",
    "        if title_url in self.used_url_title:\n",
    "            return False\n",
    "        response = self.hd.request_proxy(title_url)\n",
    "        if response:\n",
    "            dict_film_basic, df_film_crew = self.parse_title(response.content)\n",
    "            df_film_basic = self.save_title_basic(dict_film_basic)\n",
    "            \n",
    "            df_film_crew.to_sql(name=TABLENAME_FILM_CREW,con=self.conn,if_exists='append',index=False)\n",
    "            self.used_url_add(title_url, self.used_url_title, FILEPATH_USEDURL_TITLE)\n",
    "            return df_film_basic, df_film_crew\n",
    "        else:\n",
    "            self.used_url_add(title_url, self.error_url_title, FILEPATH_ERROR_TITLE)\n",
    "            print('ERROR: {}'.format(title_url))\n",
    "            return False\n",
    "    def save_title_basic(self, dict_film_basic, to_db=True, table_name=TABLENAME_TITLE_BASIC):\n",
    "        dict_for_pandas = {'ttid':[],'name':[], 'title_url':[], 'genre':[],\n",
    "                          'contentRating':[],'datePublished':[],\n",
    "                          'ratingCount':[], 'bestRating':[], 'worstRating':[], 'ratingValue':[],\n",
    "                          'budget':[], 'open_weekend_usa':[], 'gross_usa':[], 'cumulative_worldwide_gross':[],\n",
    "                          'description':[], 'keywords':[], 'plot_summary':[], 'storyline':[]}\n",
    "        for k in dict_for_pandas.keys():\n",
    "            dict_for_pandas[k].append(dict_film_basic.get(k))\n",
    "        df_film_basic = pd.DataFrame(dict_for_pandas)\n",
    "        if to_db:\n",
    "            df_film_basic.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_film_basic\n",
    "    def scrape_title_list(self, urllist, teststop=-1):\n",
    "        for i in urllist:\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                return\n",
    "            if self.scrape_title(i):\n",
    "                if teststop>0:\n",
    "                    teststop = teststop-1\n",
    "                time.sleep(1)\n",
    "        print('mission complete')\n",
    "            \n",
    "# # scrape list          \n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     sp.scrapy_li_tt_all()\n",
    "\n",
    "# scrape title\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     title_urls = df1['url'].values.tolist()\n",
    "#     sp.scrape_title_list(title_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = html_downloader(china=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = genre_url_list[0]\n",
    "t1 = imdb_spider(hd=hd)\n",
    "t1.scrapy_li_tt(url,teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t1.hd.ip_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_url = 'https://www.imdb.com/title/tt0120737/?ref_=adv_li_tt'\n",
    "r2 = hd.request_proxy(title_url)\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "d2 = t2.parse_title(r2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204484, 5)\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "cur = conn.cursor()\n",
    "df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_to_int(value):\n",
    "    s_tmp = re.search(r'\\d+',value)\n",
    "    if s_tmp:\n",
    "        tmp = s_tmp.group()\n",
    "        tmp = np.int(tmp)\n",
    "    else:\n",
    "        tmp = np.nan\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100574, 5)\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.copy()\n",
    "df2['year'] = df2['year'].apply(year_to_int)\n",
    "df2 = df2[(df2['year']>2000) & (df2['year']<2019)]\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ttid</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tt1375666</td>\n",
       "      <td>Inception</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>/title/tt1375666/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tt1706620</td>\n",
       "      <td>Snowpiercer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Action,Drama,Sci-Fi</td>\n",
       "      <td>/title/tt1706620/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tt0120737</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Action,Adventure,Drama</td>\n",
       "      <td>/title/tt0120737/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tt4779682</td>\n",
       "      <td>The Meg</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Action,Horror,Sci-Fi</td>\n",
       "      <td>/title/tt4779682/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tt1677720</td>\n",
       "      <td>Ready Player One</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>/title/tt1677720/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ttid                                               name    year  \\\n",
       "10  tt1375666                                          Inception  2010.0   \n",
       "11  tt1706620                                        Snowpiercer  2013.0   \n",
       "16  tt0120737  The Lord of the Rings: The Fellowship of the Ring  2001.0   \n",
       "20  tt4779682                                            The Meg  2018.0   \n",
       "25  tt1677720                                   Ready Player One  2018.0   \n",
       "\n",
       "                      genre                               url  \n",
       "10  Action,Adventure,Sci-Fi  /title/tt1375666/?ref_=adv_li_tt  \n",
       "11      Action,Drama,Sci-Fi  /title/tt1706620/?ref_=adv_li_tt  \n",
       "16   Action,Adventure,Drama  /title/tt0120737/?ref_=adv_li_tt  \n",
       "20     Action,Horror,Sci-Fi  /title/tt4779682/?ref_=adv_li_tt  \n",
       "25  Action,Adventure,Sci-Fi  /title/tt1677720/?ref_=adv_li_tt  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: scrape https://www.imdb.com/title/tt1375666/?ref_=adv_li_tt\n",
      "SUCCESS: scrape https://www.imdb.com/title/tt1706620/?ref_=adv_li_tt\n",
      "test end\n"
     ]
    }
   ],
   "source": [
    "t3 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "title_urls = df2['url'].values.tolist()\n",
    "t3.scrape_title_list(title_urls, teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
