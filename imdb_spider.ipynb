{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting imdb_spider_for_credit.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile imdb_spider_for_credit.py\n",
    "import requests,re,os,sqlite3,sys,time,json\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imdb_config import *\n",
    "from urllib.parse import urljoin\n",
    "from log_manager import log_manager\n",
    "sys.path.append('/home/guijideanhao/pyproject/scrapy_toolv2')\n",
    "from html_downloader import html_downloader\n",
    "\n",
    "class imdb_spider():\n",
    "    def __init__(self,log_file, dbpath=FILEPATH_DATABASE, hd=None):\n",
    "        self.dbpath = dbpath\n",
    "        self.conn = sqlite3.connect(dbpath)\n",
    "        self.cur = self.conn.cursor()\n",
    "        if hd:\n",
    "            self.hd = hd\n",
    "        else:\n",
    "            self.hd = html_downloader(china=False)\n",
    "        self.filmlist_used_ttid()\n",
    "        self.used_url_li_tt = self.used_url_gen(FILEPATH_USEDURL_LI_TT)\n",
    "        self.used_url_title = self.used_url_gen(FILEPATH_USEDURL_TITLE)\n",
    "        self.error_url_title = self.used_url_gen(FILEPATH_ERROR_TITLE)\n",
    "        self.used_ttid_connection = self.used_url_gen(FILEPATH_USEDTTID_CONNECTION)\n",
    "        self.error_ttid_connection = self.used_url_gen(FILEPATH_ERRORTTID_CONNECTION)\n",
    "        \n",
    "        self.log_manager = log_manager(log_file)\n",
    "        self.success_ttid = self.log_manager.get_info_list(success_tag='SUCCESS')\n",
    "        self.error_ttid = self.log_manager.get_info_list(success_tag='ERROR')\n",
    "    \n",
    "    def used_url_gen(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            tempstr = f.read()\n",
    "        return list(tempstr.split(','))\n",
    "    \n",
    "    def used_url_add(self, url, used_list, filename, success=True):\n",
    "        with open(filename, 'a+') as f:\n",
    "            f.write('{},'.format(url))\n",
    "        used_list.append(url)\n",
    "        if success:\n",
    "            print('SUCCESS: scrape {}'.format(url))\n",
    "        else:\n",
    "            print('ERROR: {}'.format(url))\n",
    "        \n",
    "    def filmlist_used_ttid(self):\n",
    "        try:\n",
    "            tempdf = pd.read_sql('select ttid from {}'.format(TABLENAME_FILMLIST), self.conn)\n",
    "            self.used_filmlist = set(tempdf['ttid'].values.tolist())\n",
    "        except:\n",
    "            self.used_filmlist = set()\n",
    "        \n",
    "    def parse_li_tt(self, res_content):\n",
    "        list_film_id = []\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        item_content = soup.find_all('div', {'class':'lister-item-content'})\n",
    "        for i in item_content:\n",
    "            item_header = i.find('h3', {'class':'lister-item-header'})\n",
    "            film_url = item_header.a['href']\n",
    "            film_name = item_header.a.get_text()\n",
    "            s_film_id = re.search(r'tt\\d+', film_url)\n",
    "            film_id = s_film_id.group()\n",
    "            try:\n",
    "                item_genre = i.find('span', {'class':'genre'})\n",
    "                film_genre = item_genre.get_text()\n",
    "                film_genre = re.sub(r'\\s','',film_genre)\n",
    "                film_year = i.find('span', {'class':'lister-item-year'}).get_text()\n",
    "                film_year = re.sub(r'\\D', '', film_year)\n",
    "            except:\n",
    "                print(\"ERROR: {}\".format(film_id))\n",
    "        # save\n",
    "            temp_dict = {'ttid':film_id, 'name':film_name, 'year':film_year, 'genre':film_genre, 'url':film_url}\n",
    "            list_film_id.append(temp_dict)\n",
    "        \n",
    "        # next page\n",
    "        item_next_page = soup.find('a', {'class':'lister-page-next'})\n",
    "        if item_next_page:\n",
    "            np_url = item_next_page['href']\n",
    "        else:\n",
    "            np_url = False\n",
    "        return list_film_id,np_url\n",
    "    def save_li_tt(self, list_film_id, to_db=True, table_name=TABLENAME_FILMLIST):\n",
    "        dict_for_pandas = {'ttid':[], 'name':[], 'year':[], 'genre':[], 'url':[]}\n",
    "        for i in list_film_id:\n",
    "            if i['ttid'] in self.used_filmlist:\n",
    "                continue\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(i.get(k))\n",
    "            self.used_filmlist.add(i['ttid'])\n",
    "        df_list_film = pd.DataFrame(dict_for_pandas)\n",
    "        \n",
    "        if to_db:\n",
    "            df_list_film.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_list_film\n",
    "    def scrapy_li_tt(self, genre_url,teststop=-1):\n",
    "        if teststop ==0:\n",
    "            print('test end')\n",
    "            return\n",
    "        if teststop > 0:\n",
    "            teststop = teststop-1\n",
    "        if genre_url in self.used_url_li_tt:\n",
    "            genre_url = self.used_url_li_tt[-2]\n",
    "            print('ATTENTION: start from {}'.format(genre_url))\n",
    "        response = self.hd.request_proxy(genre_url)\n",
    "        if response:\n",
    "            list_film_id,np_url = self.parse_li_tt(response.content)\n",
    "            df_list_film = self.save_li_tt(list_film_id)\n",
    "            df_list_film.to_csv(os.path.join(PATH_FILMLIST_TEMP,'{}.csv'.format(int(time.time()))), index=False)\n",
    "            self.used_url_add(genre_url,self.used_url_li_tt,FILEPATH_USEDURL_LI_TT)\n",
    "            if np_url:\n",
    "                np_url = urljoin(domain_url, np_url)\n",
    "                time.sleep(1)\n",
    "                self.scrapy_li_tt(np_url, teststop=teststop)\n",
    "        else:\n",
    "            print('ERROR: scrapy interrupt!!!')\n",
    "    \n",
    "    def scrapy_li_tt_all(self):\n",
    "        for i in genre_url_list:\n",
    "            self.scrapy_li_tt(i)\n",
    "    \n",
    "    def parse_title(self, res_content):\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        script_data = soup.find('script', {'type':'application/ld+json'})\n",
    "        if script_data:\n",
    "            dict_film_basic, df_film_crew = self.parse_title_json(script_data.get_text())\n",
    "        else:\n",
    "            dict_film_basic = {}\n",
    "            df_film_crew = pd.DataFrame()\n",
    "        plot_summary = soup.find('div',{'class':'summary_text'})\n",
    "        if plot_summary:\n",
    "            dict_film_basic['plot_summary'] = plot_summary.get_text()\n",
    "            dict_film_basic['plot_summary'] = re.sub('\\s\\s','',dict_film_basic['plot_summary'])\n",
    "        storyline = soup.find('div', {'class': 'inline canwrap'})\n",
    "        try:\n",
    "            dict_film_basic['storyline'] = storyline.p.span.get_text()\n",
    "            dict_film_basic['storyline'] = re.sub('\\s\\s','',dict_film_basic['storyline'])\n",
    "        except:\n",
    "            storyline = ''\n",
    "        \n",
    "        titleDetails = soup.find('div', {'class':'article', 'id':'titleDetails'})\n",
    "        s_budget = re.search(r'Budget:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_budget:\n",
    "            budget = s_budget.group()\n",
    "            s_budget = re.search(r'[\"$ ]+[\\d,]+',budget)\n",
    "            dict_film_basic['budget'] = s_budget.group()\n",
    "        \n",
    "        s_open_weekend_usa = re.search(r'Opening Weekend USA:</h4>[\"$ ]+[\\d,]+',str(titleDetails))\n",
    "        if s_open_weekend_usa:\n",
    "            open_weekend_usa = s_open_weekend_usa.group()\n",
    "            s_open_weekend_usa = re.search(r'[\"$ ]+[\\d,]+',open_weekend_usa)\n",
    "            dict_film_basic['open_weekend_usa'] = s_open_weekend_usa.group()\n",
    "        \n",
    "        s_gross_usa = re.search(r'Gross USA:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_gross_usa:\n",
    "            gross_usa = s_gross_usa.group()\n",
    "            s_gross_usa = re.search(r'[\"$ ]+[\\d,]+',gross_usa)\n",
    "            dict_film_basic['gross_usa'] = s_gross_usa.group()\n",
    "        \n",
    "        s_cumulative_worldwide_gross = re.search(r'Cumulative Worldwide Gross:</h4>[\"$ ]+[\\d,]+', str(titleDetails))\n",
    "        if s_cumulative_worldwide_gross:\n",
    "            cumulative_worldwide_gross = s_cumulative_worldwide_gross.group()\n",
    "            s_cumulative_worldwide_gross = re.search(r'[\"$ ]+[\\d,]+',cumulative_worldwide_gross)\n",
    "            dict_film_basic['cumulative_worldwide_gross'] = s_cumulative_worldwide_gross.group()\n",
    "        return dict_film_basic, df_film_crew\n",
    "    \n",
    "    def parse_title_json(self, json_str):\n",
    "        dict_film_basic = {}\n",
    "        tmp_dict = json.loads(json_str)\n",
    "        dict_film_basic['title_url'] = tmp_dict.get('url')\n",
    "        s_film_id = re.search(r'tt\\d+', tmp_dict.get('url'))\n",
    "        film_id = s_film_id.group()\n",
    "        dict_film_basic['ttid'] = film_id\n",
    "        dict_film_basic['name'] = tmp_dict.get('name')\n",
    "        dict_film_basic['genre'] = tmp_dict.get('genre')\n",
    "        if dict_film_basic['genre']:\n",
    "            dict_film_basic['genre'] = str(dict_film_basic['genre'])\n",
    "        dict_film_basic['contentRating'] = tmp_dict.get('contentRating')\n",
    "        dict_film_basic['description'] = tmp_dict.get('description')\n",
    "        dict_film_basic['datePublished'] = tmp_dict.get('datePublished')\n",
    "        dict_film_basic['keywords'] = tmp_dict.get('keywords')\n",
    "        agg_rating = tmp_dict.get('aggregateRating')\n",
    "        if agg_rating:\n",
    "            dict_film_basic['ratingCount'] = agg_rating.get('ratingCount')\n",
    "            dict_film_basic['bestRating'] = agg_rating.get('bestRating')\n",
    "            dict_film_basic['worstRating'] = agg_rating.get('worstRating')\n",
    "            dict_film_basic['ratingValue'] = agg_rating.get('ratingValue')\n",
    "        dict_for_pandas = {'ttid':[], 'film_name':[], 'type':[], 'person_name':[], 'person_url':[]}\n",
    "        list_film_multi = []\n",
    "        try:\n",
    "            actors = tmp_dict.get('actor')\n",
    "            if actors:\n",
    "                for i in actors:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'actor'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "            directors = tmp_dict.get('director')\n",
    "            if directors:\n",
    "                if type(directors) == dict:\n",
    "                    directors = [directors]\n",
    "                for i in directors:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'director'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "            creators = tmp_dict.get('creator')\n",
    "            if creators:\n",
    "                if type(creators) == dict:\n",
    "                    directors = [directors]\n",
    "                for i in creators:\n",
    "                    tmp = {}\n",
    "                    tmp['ttid'] = film_id\n",
    "                    tmp['film_name'] = tmp_dict.get('name')\n",
    "                    tmp['type'] = 'creator'\n",
    "                    tmp['person_name'] = i.get('name')\n",
    "                    tmp['person_url'] = i.get('url')\n",
    "                    list_film_multi.append(tmp)\n",
    "\n",
    "            for i in list_film_multi:\n",
    "                for k in i.keys():\n",
    "                    dict_for_pandas[k].append(i.get(k))\n",
    "        except:\n",
    "            self.used_url_add(tmp_dict.get('url'), self.error_url_title, FILEPATH_ERROR_TITLE)\n",
    "            print(\"ERROR: {}\".format(tmp_dict.get('url')))\n",
    "        df_film_crew = pd.DataFrame(dict_for_pandas)\n",
    "        return dict_film_basic, df_film_crew\n",
    "    def scrape_title(self, title_url):\n",
    "        title_url = urljoin(domain_url, title_url)\n",
    "        if title_url in self.used_url_title:\n",
    "            return False\n",
    "        response = self.hd.request_proxy(title_url)\n",
    "        if response:\n",
    "            dict_film_basic, df_film_crew = self.parse_title(response.content)\n",
    "            df_film_basic = self.save_title_basic(dict_film_basic)\n",
    "            \n",
    "            df_film_crew.to_sql(name=TABLENAME_FILM_CREW,con=self.conn,if_exists='append',index=False)\n",
    "            self.used_url_add(title_url, self.used_url_title, FILEPATH_USEDURL_TITLE)\n",
    "            return df_film_basic, df_film_crew\n",
    "        else:\n",
    "            self.used_url_add(title_url, self.error_url_title, FILEPATH_ERROR_TITLE)\n",
    "            print('ERROR: {}'.format(title_url))\n",
    "            return False\n",
    "    def save_title_basic(self, dict_film_basic, to_db=True, table_name=TABLENAME_TITLE_BASIC):\n",
    "        dict_for_pandas = {'ttid':[],'name':[], 'title_url':[], 'genre':[],\n",
    "                          'contentRating':[],'datePublished':[],\n",
    "                          'ratingCount':[], 'bestRating':[], 'worstRating':[], 'ratingValue':[],\n",
    "                          'budget':[], 'open_weekend_usa':[], 'gross_usa':[], 'cumulative_worldwide_gross':[],\n",
    "                          'description':[], 'keywords':[], 'plot_summary':[], 'storyline':[]}\n",
    "        for k in dict_for_pandas.keys():\n",
    "            dict_for_pandas[k].append(dict_film_basic.get(k))\n",
    "        df_film_basic = pd.DataFrame(dict_for_pandas)\n",
    "        if to_db:\n",
    "            df_film_basic.to_sql(name=table_name,con=self.conn,if_exists='append',index=False)\n",
    "        return df_film_basic\n",
    "    def scrape_title_list(self, urllist, teststop=-1):\n",
    "        for i in urllist:\n",
    "            if teststop==0:\n",
    "                print('test end')\n",
    "                return\n",
    "            if self.scrape_title(i):\n",
    "                if teststop>0:\n",
    "                    teststop = teststop-1\n",
    "                time.sleep(1)\n",
    "        print('mission complete')\n",
    "        \n",
    "    def parse_connection(self, res_content, film_ttid):\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        connection_content = soup.find('div', {'id':'connections_content'})\n",
    "        connection_list = connection_content.find('div', {'class':'list'})\n",
    "        connections = connection_list.contents\n",
    "        list_connection_data = []\n",
    "        ttype_list = ['TV Episode', 'Video', 'TV Movie', 'Short', 'TV Series']\n",
    "        for i in connections:\n",
    "            if not isinstance(i, NavigableString):\n",
    "                if i.name == 'a':\n",
    "                    tmp_type = i['id']\n",
    "                if i.name == 'div' and 'soda' in i['class']:\n",
    "                    tmp = i.contents\n",
    "                    tmp_dict = {'film_ttid':film_ttid, 'type':tmp_type}\n",
    "                    for j in tmp:\n",
    "                        if  j.name == 'a':\n",
    "                            tmp_name = j.get_text()\n",
    "                            tmp_name = re.sub(r'[\\s]',' ',tmp_name)\n",
    "                            if not tmp_dict.get('name'):\n",
    "                                tmp_dict['name'] = tmp_name\n",
    "                            tmp_url = j.get('href')\n",
    "                            if tmp_url:\n",
    "                                tmp_ttid = tmp_url.replace('/title/','')\n",
    "                                if not tmp_dict.get('url'):\n",
    "                                    tmp_dict['url'] = tmp_url\n",
    "                                    tmp_dict['ttid'] = tmp_ttid\n",
    "                            \n",
    "                        else:\n",
    "                            if re.search(r'\\d{4}', str(j)):\n",
    "                                s_tmp_date = re.search(r'\\d{4}', str(j))\n",
    "                                tmp_date = s_tmp_date.group()\n",
    "                                if not tmp_dict.get('year'):\n",
    "                                    tmp_dict['year'] = tmp_date\n",
    "                            for h in ttype_list:\n",
    "                                if h in str(j):\n",
    "                                    if not tmp_dict.get('ttype'):\n",
    "                                        tmp_dict['ttype'] = h\n",
    "                                \n",
    "                    list_connection_data.append(tmp_dict)\n",
    "        dict_for_pandas = {'film_ttid':[], 'type':[], 'name':[], 'ttid':[], 'url':[], 'year':[], 'ttype':[]}\n",
    "        for h in list_connection_data:\n",
    "            for k in dict_for_pandas.keys():\n",
    "                dict_for_pandas[k].append(h.get(k))\n",
    "        df_connections = pd.DataFrame(dict_for_pandas)\n",
    "                \n",
    "        return df_connections\n",
    "    def scrapy_connections(self, ttid):\n",
    "        url = 'https://www.imdb.com/title/{}/movieconnections?ref_=tt_ql_trv_6'.format(ttid)\n",
    "        if ttid in self.used_ttid_connection:\n",
    "            return False\n",
    "        response = self.hd.request_proxy(url)\n",
    "        if response:\n",
    "            try:\n",
    "                df_connections = self.parse_connection(response.content, ttid)\n",
    "                df_connections.to_sql(name=TABLENAME_FILE_CONNECTIONS,con=self.conn,if_exists='append',index=False)\n",
    "                self.used_url_add(ttid, self.used_ttid_connection, FILEPATH_USEDTTID_CONNECTION)\n",
    "                return True,df_connections\n",
    "            except:\n",
    "                self.used_url_add(ttid, self.error_ttid_connection, FILEPATH_ERRORTTID_CONNECTION, success=False)\n",
    "                return False\n",
    "        else:\n",
    "            print('ERROR: {}'.format(ttid))\n",
    "            return False\n",
    "    def scrapy_connections_list(self, ttid_list, teststop=-1):\n",
    "        for i in ttid_list:\n",
    "            if teststop == 0:\n",
    "                print('test end')\n",
    "                break\n",
    "            if self.scrapy_connections(i):\n",
    "                if teststop>0:\n",
    "                    teststop = teststop -1\n",
    "        print('mission complete')\n",
    "    def parse_company_credit(self, res_content, film_ttid):\n",
    "        '''\n",
    "        for parse company credit page\n",
    "        Args:\n",
    "            res_content (byte): the response content\n",
    "            film_ttid (str): the film id in imdb.com\n",
    "        Returns:\n",
    "            DataFrame, a dataframe of the firms credit\n",
    "        '''\n",
    "        soup = BeautifulSoup(res_content, 'html.parser')\n",
    "        co_credit_content = soup.find('div', {'id': 'company_credits_content'})\n",
    "        # no need to confirm\n",
    "        production = co_credit_content.find('h4', {'id': 'production'})\n",
    "        distributors = co_credit_content.find('h4', {'id': 'distributors'})\n",
    "        # confirm before parse\n",
    "        other = co_credit_content.find('h4', {'id':'other'})\n",
    "        \n",
    "        tmp_data_list = []\n",
    "        for i in production.next_siblings:\n",
    "            if not isinstance(i, NavigableString):\n",
    "                if i.name == 'ul':\n",
    "                    production_ul = i\n",
    "        tmp_data_list = tmp_data_list + self.parse_company_credit_ul(production_ul, 'production_company')\n",
    "        for i in distributors.next_siblings:\n",
    "            if not isinstance(i, NavigableString):\n",
    "                if i.name == 'ul':\n",
    "                    distributors_ul = i \n",
    "        tmp_data_list = tmp_data_list + self.parse_company_credit_ul(distributors_ul, 'distributors')\n",
    "        if other:\n",
    "            for i in other.next_siblings:\n",
    "                if not isinstance(i, NavigableString):\n",
    "                    if i.name == 'ul':\n",
    "                        other_ul = i        \n",
    "            tmp_data_list = tmp_data_list + self.parse_company_credit_ul(other_ul, 'other_companys')\n",
    "\n",
    "        \n",
    "        # list to dataframe\n",
    "        cols = ['company_credit', 'company_id', 'company_name', 'note']\n",
    "        dict_for_pandas = {}\n",
    "        for i in cols:\n",
    "            dict_for_pandas[i] = []\n",
    "        for i in tmp_data_list:\n",
    "            for j in cols:\n",
    "                dict_for_pandas[j].append(i.get(j))\n",
    "        tmp_df = pd.DataFrame(dict_for_pandas)\n",
    "        tmp_df['ttid'] = film_ttid\n",
    "        cols_rearrange = ['ttid'] + cols\n",
    "        tmp_df = tmp_df[cols_rearrange]\n",
    "        return tmp_df\n",
    "    def parse_company_credit_ul(self, ul_element, company_credit):\n",
    "        '''\n",
    "        to parse the ul element in company credit page\n",
    "        \n",
    "        Args:\n",
    "            ul_element (object): BeautifulSoup object of ul element\n",
    "            company_credit (str): the type of company_credit, (production_company, distributors or other_companys)\n",
    "        Returns:\n",
    "            list, a list of dict\n",
    "            \n",
    "        '''\n",
    "        tmp_data_list = []\n",
    "        for i in ul_element.find_all('li'):\n",
    "            company_id = i.find('a')['href']\n",
    "            company_id = re.sub(r'\\?[\\S\\s]*','', company_id)\n",
    "            company_name = i.find('a').get_text()\n",
    "            note = i.find('a').next_sibling\n",
    "            note = note.strip()\n",
    "            tmp_item = {'company_credit':company_credit,'company_id': company_id, 'company_name': company_name, 'note': note}\n",
    "            tmp_data_list.append(tmp_item)\n",
    "        return tmp_data_list\n",
    "\n",
    "    def scrapy_company_credit(self, film_ttid):\n",
    "        '''\n",
    "        Args:\n",
    "            film_ttid (str)\n",
    "        '''\n",
    "        if film_ttid in self.error_ttid + self.success_ttid:\n",
    "            return\n",
    "        else:\n",
    "            # gen url \n",
    "            url = 'https://www.imdb.com/title/{}/companycredits?ref_=tt_ql_dt_4'.format(film_ttid)\n",
    "            try:\n",
    "                res = self.hd.request_proxy(url)\n",
    "                if res == None:\n",
    "                    self.log_manager.write_log(film_ttid, success=False, info_type='download',printout=True, add_to_list=self.error_ttid)\n",
    "                    return\n",
    "            except:\n",
    "                self.log_manager.write_log(film_ttid, success=False, info_type='download',printout=True, add_to_list=self.error_ttid)\n",
    "                return\n",
    "            try:\n",
    "                df = self.parse_company_credit(res.content, film_ttid)\n",
    "            except:\n",
    "                self.log_manager.write_log(film_ttid, success=False, info_type='parse',printout=True, add_to_list=self.error_ttid)\n",
    "                return\n",
    "            try:\n",
    "                df.to_sql(name=TABLENAME_FILE_COMPANYCREDIT,con=self.conn,if_exists='append',index=False)\n",
    "                self.log_manager.write_log(film_ttid, success=True, info_type='saved',printout=True, add_to_list=self.error_ttid)\n",
    "                return df\n",
    "            except:\n",
    "                self.log_manager.write_log(film_ttid, success=False, info_type='db',printout=True, add_to_list=self.error_ttid)\n",
    "                return\n",
    "    def scrapy_company_credit_list(self, ttid_list, teststop=-1):\n",
    "        for i in ttid_list:\n",
    "            if teststop == 0:\n",
    "                print('test end')\n",
    "                break\n",
    "                \n",
    "            df = self.scrapy_company_credit(i)\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                if teststop > 0:\n",
    "                    teststop = teststop-1\n",
    "        print('mission complete')\n",
    "    def parse_user_review\n",
    "            \n",
    "# # scrape list          \n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     sp.scrapy_li_tt_all()\n",
    "\n",
    "# scrape title\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     title_urls = df1['url'].values.tolist()\n",
    "#     sp.scrape_title_list(title_urls)\n",
    "\n",
    "# # scrape connections\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider()\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     ttids = df1['ttid'].values.tolist()\n",
    "#     sp.scrapy_connections_list(ttids)\n",
    "\n",
    "# # scrape company credit\n",
    "# if __name__ == \"__main__\":\n",
    "#     sp = imdb_spider(log_file=FILEPATH_LOGFILE_COMPANY_CREDIT, dbpath=FILEPATH_DATABASE3)\n",
    "#     conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "#     cur = conn.cursor()\n",
    "#     df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "    \n",
    "#     def year_to_int(value):\n",
    "#         s_tmp = re.search(r'\\d+',value)\n",
    "#         if s_tmp:\n",
    "#             tmp = s_tmp.group()\n",
    "#             tmp = np.int(tmp)\n",
    "#         else:\n",
    "#             tmp = np.nan\n",
    "#         return tmp\n",
    "#     df1['year'] = df1['year'].apply(year_to_int)\n",
    "#     df1 = df1[(df1['year']>2000) & (df1['year']<2019)]\n",
    "#     ttids = df1['ttid'].values.tolist()\n",
    "#     sp.scrapy_company_credit_list(ttids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perpare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd = html_downloader(china=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the len of list: 4\n",
      "the len of list: 0\n"
     ]
    }
   ],
   "source": [
    "test = imdb_spider(log_file = 'testlog_company_credit.txt', dbpath='test.db',hd=hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = genre_url_list[0]\n",
    "t1 = imdb_spider(hd=hd)\n",
    "t1.scrapy_li_tt(url,teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t1.hd.ip_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_url = 'https://www.imdb.com/title/tt0120737/?ref_=adv_li_tt'\n",
    "r2 = hd.request_proxy(title_url)\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "d2 = t2.parse_title(r2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1294970, 5)\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(FILEPATH_DATABASE)\n",
    "cur = conn.cursor()\n",
    "df1 = pd.read_sql('select * from {}'.format(TABLENAME_FILMLIST), conn)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_to_int(value):\n",
    "    s_tmp = re.search(r'\\d+',value)\n",
    "    if s_tmp:\n",
    "        tmp = s_tmp.group()\n",
    "        tmp = np.int(tmp)\n",
    "    else:\n",
    "        tmp = np.nan\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(707881, 5)\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.copy()\n",
    "df2['year'] = df2['year'].apply(year_to_int)\n",
    "df2 = df2[(df2['year']>2000) & (df2['year']<2019)]\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ttid</th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>genre</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tt1375666</td>\n",
       "      <td>Inception</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>/title/tt1375666/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tt1706620</td>\n",
       "      <td>Snowpiercer</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Action,Drama,Sci-Fi</td>\n",
       "      <td>/title/tt1706620/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tt0120737</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Action,Adventure,Drama</td>\n",
       "      <td>/title/tt0120737/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tt4779682</td>\n",
       "      <td>The Meg</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Action,Horror,Sci-Fi</td>\n",
       "      <td>/title/tt4779682/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tt1677720</td>\n",
       "      <td>Ready Player One</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Action,Adventure,Sci-Fi</td>\n",
       "      <td>/title/tt1677720/?ref_=adv_li_tt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ttid                                               name    year  \\\n",
       "10  tt1375666                                          Inception  2010.0   \n",
       "11  tt1706620                                        Snowpiercer  2013.0   \n",
       "16  tt0120737  The Lord of the Rings: The Fellowship of the Ring  2001.0   \n",
       "20  tt4779682                                            The Meg  2018.0   \n",
       "25  tt1677720                                   Ready Player One  2018.0   \n",
       "\n",
       "                      genre                               url  \n",
       "10  Action,Adventure,Sci-Fi  /title/tt1375666/?ref_=adv_li_tt  \n",
       "11      Action,Drama,Sci-Fi  /title/tt1706620/?ref_=adv_li_tt  \n",
       "16   Action,Adventure,Drama  /title/tt0120737/?ref_=adv_li_tt  \n",
       "20     Action,Horror,Sci-Fi  /title/tt4779682/?ref_=adv_li_tt  \n",
       "25  Action,Adventure,Sci-Fi  /title/tt1677720/?ref_=adv_li_tt  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = imdb_spider(dbpath='test.db',hd=hd)\n",
    "title_urls = df2['url'].values.tolist()\n",
    "t3.scrape_title_list(title_urls, teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209579, 18)\n"
     ]
    }
   ],
   "source": [
    "conn2 = sqlite3.connect(FILEPATH_DATABASE2)\n",
    "dftitle_basic = pd.read_sql('select * from {}'.format(TABLENAME_TITLE_BASIC), conn2)\n",
    "dftitle_crew = pd.read_sql('select * from {}'.format(TABLENAME_FILM_CREW), conn2)\n",
    "print(dftitle_basic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftitle_basic.to_csv('/home/guijideanhao/buffer/data_preview_film_basic.csv', index=False)\n",
    "dftitle_crew.to_csv('/home/guijideanhao/buffer/data_preview_film_crew.csv', index=False)\n",
    "df2.to_csv('/home/guijideanhao/buffer/data_preview_film_list2000-2019part.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connection page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_url = 'https://www.imdb.com/title/tt1320253/movieconnections?ref_=tt_ql_trv_6'\n",
    "r4 = requests.get(connect_url)\n",
    "r4.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = imdb_spider(log_file = 'testlog_company_credit.txt', dbpath='test.db',hd=hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = t4.parse_connection(r4.content, 'tt1320253')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttids = df2['ttid'].values.tolist()\n",
    "t4.scrapy_connections_list(ttids, teststop=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_connections = pd.read_sql('select * from {}'.format(TABLENAME_FILE_CONNECTIONS), t4.conn)\n",
    "print(df_connections.shape)\n",
    "df_connections.to_csv('/home/guijideanhao/buffer/data_preview_film_connections.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## company credit page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_credit_url = 'https://www.imdb.com/title/tt4520988/companycredits?ref_=tt_ql_dt_4'\n",
    "r5 = hd.request_proxy(company_credit_url)\n",
    "r5.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_credit.html', 'ab+') as f:\n",
    "    f.write(r5.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = test.parse_company_credit(r5.content, 'tt4520988')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = test.scrapy_company_credit('tt4520988')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.scrapy_company_credit_list(df2['ttid'].values.tolist(), teststop=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39827\n"
     ]
    }
   ],
   "source": [
    "conn5 = sqlite3.connect(FILEPATH_DATABASE3)\n",
    "df_credit = pd.read_sql('select * from {}'.format(TABLENAME_FILE_COMPANYCREDIT), conn5)\n",
    "print(len(set(df_credit['ttid'].values.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
